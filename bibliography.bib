
@article{freedman_histogram_1981,
	title = {On the histogram as a density estimator:{L} 2 theory},
	volume = {57},
	copyright = {http://www.springer.com/tdm},
	issn = {0044-3719, 1432-2064},
	shorttitle = {On the histogram as a density estimator},
	url = {https://doi.org/10.1007/BF01025868},
	doi = {10.1007/BF01025868},
	language = {en},
	number = {4},
	urldate = {2024-04-12},
	journal = {Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete},
	author = {Freedman, David and Diaconis, Persi},
	month = dec,
	year = {1981},
	pages = {453--476},
}

@book{gelman_bayesian_2013,
	edition = {0},
	title = {Bayesian Data Analysis},
	isbn = {978-0-429-11307-9},
	url = {https://doi.org/10.1201/b16018},
	language = {en},
	urldate = {2024-02-23},
	publisher = {Chapman and Hall/CRC},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	month = nov,
	year = {2013},
	doi = {10.1201/b16018},
}

@article{strumbelj_past_2024,
author = {Erik Štrumbelj and Alexandre Bouchard-C{\^o}t{\'e} and Jukka Corander and Andrew Gelman and H{\aa}vard Rue and Lawrence Murray and Henri Pesonen and Martyn Plummer and Aki Vehtari},
title = {{Past, Present and Future of Software for Bayesian Inference}},
volume = {39},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {46 -- 61},
keywords = {computation, data analysis, MCMC, probabilistic programming, statistics},
year = {2024},
doi = {10.1214/23-STS907},
URL = {https://doi.org/10.1214/23-STS907}
}


@article{box_sampling_1980,
	title = {Sampling and {Bayes}' Inference in Scientific Modelling and Robustness},
	volume = {143},
	issn = {00359238},
	url = {https://doi.org/10.2307/2982063},
	doi = {10.2307/2982063},
	number = {4},
	urldate = {2024-05-23},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	author = {Box, George E. P.},
	year = {1980},
	pages = {383},
}

@article{rubin_bayesianly_1984,
	title = {Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician},
	volume = {12},
	issn = {0090-5364},
	url = {https://doi.org/10.1214/aos/1176346785},
	doi = {10.1214/aos/1176346785},
	number = {4},
	urldate = {2024-05-23},
	journal = {The Annals of Statistics},
	author = {Rubin, Donald B.},
	month = dec,
	year = {1984},
}

@article{JMLR:v25:19-556,
	title = {Pareto smoothed importance sampling},
	volume = {25},
	url = {http://jmlr.org/papers/v25/19-556.html},
	number = {72},
	journal = {Journal of Machine Learning Research},
	author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
	year = {2024},
	pages = {1--58},
}

@book{wickham_ggplot2_2016,
	title = {{ggplot2}: Elegant Graphics for Data Analysis},
	isbn = {978-3-319-24277-4},
	publisher = {Springer-Verlag New York},
	author = {Wickham, Hadley},
	year = {2016},
	url = {https://doi.org/10.1007/978-3-319-24277-4}
}

@inproceedings{niculescu-mizil_predicting_2005,
	address = {Bonn, Germany},
	title = {Predicting good probabilities with supervised learning},
	isbn = {978-1-59593-180-1},
	url = {http://doi.org/10.1145/1102351.1102430},
	doi = {10.1145/1102351.1102430},
	language = {en},
	urldate = {2024-02-23},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning  - {ICML} '05},
	publisher = {ACM Press},
	author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
	year = {2005},
	pages = {625--632},
}

@article{degroot_comparison_1983,
	title = {The Comparison and Evaluation of Forecasters},
	volume = {32},
	issn = {00390526},
	url = {https://doi.org/10.2307/2987588},
	doi = {10.2307/2987588},
	number = {1/2},
	urldate = {2024-02-23},
	journal = {The Statistician},
	author = {DeGroot, Morris H. and Fienberg, Stephen E.},
	month = mar,
	year = {1983},
	pages = {12},
}

@book{agresti_categorical_2013,
	title = {Categorical data analysis},
	author = {Agresti, Alan},
	address = {Hoboken, New Jersey},
	edition = {Third edition},
	series = {Wiley series in probability and statistics},
	isbn = {978-0-470-46363-5},
	abstract = {"A classic in its own right, this book continues to provide an introduction to modern generalized linear models for categorical variables. The text emphasizes methods that are most commonly used in practical application, such as classical inferences for two- and three-way contingency tables, logistic regression, loglinear models, models for multinomial (nominal and ordinal) responses, and methods for repeated measurement and other forms of clustered, correlated response data. Chapter headings remain essentially with the exception of a new one on Bayesian inference for parametric models. Other major changes include an expansion of clustered data, new research on analysis of data sets with robust variables, extensive discussions of ordinal data, more on interpretation, and additional exercises throughout the book. R and SAS are now showcased as the software of choice. An author web site with solutions, commentaries, software programs, and data sets is available"--},
	language = {eng},
	publisher = {Wiley-Interscience},
	year = {2013},
}

@incollection{tukey_graphic_1972,
	address = {Ames, Iowa},
	title = {Some Graphic and Semi-Graphic Displays},
	booktitle = {Statistical Papers in Honor of {George} {W}. {Snedecor}},
	publisher = {Iowa State University Press},
	author = {Tukey, John Wilder},
	editor = {Bancroft, T. A. and Brown, S. A.},
	year = {1972},
	pages = {293--316},
}

@book{gelman_regression_2020,
	edition = {1},
	title = {Regression and Other Stories},
	isbn = {978-1-139-16187-9 978-1-107-02398-7 978-1-107-67651-0},
	url = {https://doi.org/10.1017/9781139161879},
	abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
	urldate = {2024-02-20},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
	month = jul,
	year = {2020},
	doi = {10.1017/9781139161879},
}

@article{ayer_empirical_1955,
	title = {An Empirical Distribution Function for Sampling with Incomplete Information},
	volume = {26},
	issn = {0003-4851},
	url = {http://doi.org/10.1214/aoms/1177728423},
	doi = {10.1214/aoms/1177728423},
	language = {en},
	number = {4},
	urldate = {2024-02-19},
	journal = {The Annals of Mathematical Statistics},
	author = {Ayer, Miriam and Brunk, H. D. and Ewing, G. M. and Reid, W. T. and Silverman, Edward},
	month = dec,
	year = {1955},
	pages = {641--647},
}

@misc{package_ggdist,
	title = {{ggdist}: Visualizations of Distributions and Uncertainty},
	copyright = {Open Access},
	shorttitle = {ggdist},
	url = {https://mjskay.github.io/ggdist/},
	urldate = {2023-04-12},
	publisher = {Zenodo},
	author = {Kay, Matthew},
	year = {2023},
	doi = {10.5281/zenodo.3879620},
}

@Article{kay_ggdist_2023,
    author = {Matthew Kay},
    title = {{ggdist}: Visualizations of Distributions and Uncertainty in the Grammar of Graphics},
    journal = {IEEE Transactions on Visualization and Computer Graphics},
    year = {2024},
    volume = {30},
    number = {1},
    pages = {414--424},
    doi = {10.1109/TVCG.2023.3327195},
  }


@article{kleiber_visualizing_2016,
	title = {Visualizing Count Data Regressions Using Rootograms},
	volume = {70},
	issn = {0003-1305, 1537-2731},
	url = {https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1173590},
	doi = {10.1080/00031305.2016.1173590},
	abstract = {The rootogram is a graphical tool associated with the work of J. W. Tukey that was originally used for assessing goodness of fit of univariate distributions. Here, we extend the rootogram to regression models and show that this is particularly useful for diagnosing and treating issues such as overdispersion and/or excess zeros in count data models. We also introduce a weighted version of the rootogram that can be applied out of sample or to (weighted) subsets of the data, for example, in finite mixture models. An empirical illustration revisiting a well-known dataset from ethology is included, for which a negative binomial hurdle model is employed. Supplementary materials providing two further illustrations are available online: the first, using data from public health, employs a two-component finite mixture of negative binomial models; the second, using data from finance, involves underdispersion. An R implementation of our tools is available in the R package countreg. It also contains the data and replication code.},
	language = {en},
	number = {3},
	urldate = {2024-01-29},
	journal = {The American Statistician},
	author = {Kleiber, Christian and Zeileis, Achim},
	month = jul,
	year = {2016},
	keywords = {Finite mixture, Goodness of fit, Hurdle model, Negative binomial regression, Poisson regression},
	pages = {296--303},
}

@article{fruiiwirth-schnatter_recursive_1996,
	title = {Recursive residuals and model diagnostics for normal and non-normal state space models},
	volume = {3},
	issn = {1352-8505, 1573-3009},
	url = {http://link.springer.com/10.1007/BF00539368},
	doi = {10.1007/BF00539368},
	language = {en},
	number = {4},
	urldate = {2024-01-12},
	journal = {Environmental and Ecological Statistics},
	author = {Früiiwirth-Schnatter, Sylvia},
	month = dec,
	year = {1996},
	pages = {291--309},
}

@article{wiesmann_effect_1975,
	title = {Effect of chloroquine on cultured fibroblasts: release of lysosomal hydrolases and inhibition of their uptake},
	volume = {66},
	issn = {1090-2104},
	shorttitle = {Effect of chloroquine on cultured fibroblasts},
	doi = {10.1016/0006-291x(75)90506-9},
	language = {eng},
	number = {4},
	journal = {Biochemical and Biophysical Research Communications},
	author = {Wiesmann, U. N. and DiDonato, S. and Herschkowitz, N. N.},
	month = oct,
	year = {1975},
	pmid = {4},
	keywords = {Biological Transport, Cells, Cultured, Cerebroside-Sulfatase, Chloroquine, Dextrans, Fibroblasts, Glucuronidase, Humans, Leukodystrophy, Metachromatic, Lysosomes, Pinocytosis, Skin, Sulfatases},
	pages = {1338--1343},
}

@article{liu_statistical_2019,
	title = {Statistical Analysis of Zero-Inflated Nonnegative Continuous Data: A Review},
	volume = {34},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Statistical {Analysis} of {Zero}-{Inflated} {Nonnegative} {Continuous} {Data}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-34/issue-2/Statistical-Analysis-of-Zero-Inflated-Nonnegative-Continuous-Data--A/10.1214/18-STS681.full},
	doi = {10.1214/18-STS681},
	abstract = {Zero-inflated nonnegative continuous (or semicontinuous) data arise frequently in biomedical, economical, and ecological studies. Examples include substance abuse, medical costs, medical care utilization, biomarkers (e.g., CD4 cell counts, coronary artery calcium scores), single cell gene expression rates, and (relative) abundance of microbiome. Such data are often characterized by the presence of a large portion of zero values and positive continuous values that are skewed to the right and heteroscedastic. Both of these features suggest that no simple parametric distribution may be suitable for modeling such type of outcomes. In this paper, we review statistical methods for analyzing zero-inflated nonnegative outcome data. We will start with the cross-sectional setting, discussing ways to separate zero and positive values and introducing flexible models to characterize right skewness and heteroscedasticity in the positive values. We will then present models of correlated zero-inflated nonnegative continuous data, using random effects to tackle the correlation on repeated measures from the same subject and that across different parts of the model. We will also discuss expansion to related topics, for example, zero-inflated count and survival data, nonlinear covariate effects, and joint models of longitudinal zero-inflated nonnegative continuous data and survival. Finally, we will present applications to three real datasets (i.e., microbiome, medical costs, and alcohol drinking) to illustrate these methods. Example code will be provided to facilitate applications of these methods.},
	number = {2},
	urldate = {2024-01-08},
	journal = {Statistical Science},
	author = {Liu, Lei and Shih, Ya-Chen Tina and Strawderman, Robert L. and Zhang, Daowen and Johnson, Bankole A. and Chai, Haitao},
	month = may,
	year = {2019},
	keywords = {Tobit model, cure rate, frailty model, health econometrics, joint model, semiparametric regression, splines, two-part model},
	pages = {253--279},
}

@article{van_zwet_significance_2021,
	title = {The significance filter, the winner's curse and the need to shrink},
	volume = {75},
	issn = {0039-0402, 1467-9574},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/stan.12241},
	doi = {10.1111/stan.12241},
	abstract = {The “significance filter” refers to focusing exclusively on statistically significant results. Since frequentist properties such as unbiasedness and coverage are valid only before the data have been observed, there are no guarantees if we condition on significance. In fact, the significance filter leads to overestimation of the magnitude of the parameter, which has been called the “winner's curse.” It can also lead to undercoverage of the confidence interval. Moreover, these problems become more severe if the power is low. These issues clearly deserve our attention. They have been studied mostly through empirical observation and simulation, while there are relatively few mathematical results. Here we study them both from the frequentist and the Bayesian perspective. We prove that the relative bias of the magnitude is a decreasing function of the power and that the usual confidence interval undercovers when the power is less than 50\%. We conclude that it is important to apply the appropriate amount of shrinkage to counter the winner's curse.},
	language = {en},
	number = {4},
	urldate = {2023-12-22},
	journal = {Statistica Neerlandica},
	author = {Van Zwet, Erik W. and Cator, Eric A.},
	month = nov,
	year = {2021},
	pages = {437--452},
}

@book{silverman_density_1986,
  title = {Density Estimation for Statistics and Data Analysis},
  author = {Silverman, B. W.},
  year = {2018},
  series = {Chapman and {{Hall}}/{{CRC Monographs}} on {{Statistics}} and {{Applied Probability}}},
  number = {v.26},
  publisher = {Routledge},
  address = {Boca Raton},
  isbn = {978-0-412-24620-3 978-1-351-45617-3},
  langid = {english}
}


@book{scott_multivariate_1992,
	title = {Multivariate Density Estimation: Theory, Practice, and Visualization},
	author = {Scott, David W.},
	edition = {1},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	isbn = {9780471547709 9780470316849},
	shorttitle = {Multivariate {Density} {Estimation}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316849},
	language = {en},
	urldate = {2023-07-11},
	publisher = {Wiley},
	month = aug,
	year = {1992},
	doi = {10.1002/9780470316849},
}

@article{sheather_reliable_1991,
	title = {A Reliable Data-Based Bandwidth Selection Method for Kernel Density Estimation},
	author = {Sheather, S. J. and Jones, M. C.},
	volume = {53},
	issn = {00359246},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2517-6161.1991.tb01857.x},
	doi = {10.1111/j.2517-6161.1991.tb01857.x},
	language = {en},
	number = {3},
	urldate = {2023-07-11},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	month = jul,
	year = {1991},
	pages = {683--690},
}

@article{wesnerChoosingPriorsBayesian2021,
  title = {Choosing Priors in {Bayesian} Ecological Models by Simulating from the Prior Predictive Distribution},
  author = {Wesner, Jeff S. and Pomeranz, Justin P. F.},
  year = {2021},
  month = sep,
  journal = {Ecosphere},
  volume = {12},
  number = {9},
  pages = {e03739},
  issn = {2150-8925, 2150-8925},
  doi = {10.1002/ecs2.3739},
  urldate = {2024-11-28},
  abstract = {Abstract             Bayesian data analysis is increasingly used in ecology, but prior specification remains focused on choosing non-informative priors (e.g., flat or vague priors). One barrier to choosing more informative priors is that priors must be specified on model parameters (e.g., intercepts, slopes, and sigmas), but prior knowledge often exists on the level of the response variable. This is particularly true for common models in ecology, like generalized linear mixed models that have a link function and potentially dozens of parameters, each of which needs a prior distribution. We suggest that this difficulty can be overcome by simulating from the prior predictive distribution and visualizing the results on the scale of the response variable. In doing so, some common choices for non-informative priors on parameters can easily be seen to produce biologically impossible values of response variables. Such implications of prior choices are difficult to foresee without visualization. We demonstrate a workflow for prior selection using simulation and visualization with two ecological examples (predator--prey body sizes and spider responses to food competition). This approach is not new, but its adoption by ecologists will help to better incorporate prior information in ecological models, thereby maximizing one of the benefits of Bayesian data analysis.},
  langid = {english},
  file = {C:\Users\teemu\Zotero\storage\ZMLJJQJC\Wesner and Pomeranz - 2021 - Choosing priors in Bayesian ecological models by simulating from the prior predictive distribution.pdf}
}


@article{sailynoja_graphical_2022,
	title = {Graphical test for discrete uniformity and its applications in goodness-of-fit evaluation and multiple sample comparison},
	author = {Säilynoja, Teemu and Bürkner, Paul-Christian and Vehtari, Aki},
	volume = {32},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-022-10090-6},
	doi = {10.1007/s11222-022-10090-6},
	abstract = {Assessing goodness of fit to a given distribution plays an important role in computational statistics. The probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation- and optimization-based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample, which is useful, for example, for simulation-based calibration. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is useful, for example, as a complementary diagnostic in multi-chain Markov chain Monte Carlo (MCMC) convergence diagnostics, where most currently used convergence diagnostics provide a single diagnostic value, but do not usually offer insight into the nature of the deviation. We provide numerical experiments to assess the properties of the tests using both simulated and real-world data and give recommendations on their practical application in computational statistics workflows.},
	language = {en},
	number = {2},
	journal = {Statistics and Computing},
	month = mar,
	year = {2022},
	keywords = {ECDF, MCMC convergence diagnostic, PIT, Simulation-based calibration, Uniformity test},
	pages = {32},
}

@article{kumar_arviz_2019,
	title = {{ArviZ} a unified library for exploratory analysis of {Bayesian} models in {Python}},
	volume = {4},
	issn = {2475-9066},
	url = {http://joss.theoj.org/papers/10.21105/joss.01143},
	doi = {10.21105/joss.01143},
	number = {33},
	urldate = {2023-04-18},
	journal = {Journal of Open Source Software},
	author = {Kumar, Ravin and Carroll, Colin and Hartikainen, Ari and Martin, Osvaldo},
	month = jan,
	year = {2019},
	pages = {1143},
}

@article{dimitriadis_stable_2021,
	title = {Stable reliability diagrams for probabilistic classifiers},
	volume = {118},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.2016191118},
	doi = {10.1073/pnas.2016191118},
	abstract = {Significance 
            Probabilistic classifiers assign predictive probabilities to binary events, such as rainfall tomorrow, a recession, or a personal health outcome. Such a system is reliable or calibrated if the predictive probabilities are matched by the observed frequencies. In practice, calibration is assessed graphically in reliability diagrams and quantified via the reliability component of mean scores. Extant approaches rely on binning and counting and have been hampered by ad hoc implementation decisions, a lack of reproducibility, and inefficiency. Here, we introduce the CORP approach, which uses the pool-adjacent-violators algorithm to generate optimally binned, reproducible, and provably statistically consistent reliability diagrams, along with a numerical measure of miscalibration based on a revisited score decomposition. 
          ,  
            A probability forecast or probabilistic classifier is reliable or calibrated if the predicted probabilities are matched by ex post observed frequencies, as examined visually in reliability diagrams. The classical binning and counting approach to plotting reliability diagrams has been hampered by a lack of stability under unavoidable, ad hoc implementation decisions. Here, we introduce the CORP approach, which generates provably statistically consistent, optimally binned, and reproducible reliability diagrams in an automated way. CORP is based on nonparametric isotonic regression and implemented via the pool-adjacent-violators (PAV) algorithm—essentially, the CORP reliability diagram shows the graph of the PAV-(re)calibrated forecast probabilities. The CORP approach allows for uncertainty quantification via either resampling techniques or asymptotic theory, furnishes a numerical measure of miscalibration, and provides a CORP-based Brier-score decomposition that generalizes to any proper scoring rule. We anticipate that judicious uses of the PAV algorithm yield improved tools for diagnostics and inference for a very wide range of statistical and machine learning methods.},
	language = {en},
	number = {8},
	urldate = {2023-04-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Dimitriadis, Timo and Gneiting, Tilmann and Jordan, Alexander I.},
	month = feb,
	year = {2021},
	pages = {e2016191118},
}

@article{gabry_visualization_2019,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	issn = {0964-1998, 1467-985X},
	url = {http://arxiv.org/abs/1709.01449},
	doi = {10.1111/rssa.12378},
	abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers.},
	number = {2},
	urldate = {2021-11-17},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	month = feb,
	year = {2019},
	note = {tex.ids= gabryVisualizationBayesianWorkflow2019a
arXiv: 1709.01449},
	keywords = {Statistics - Applications, Statistics - Methodology},
	pages = {389--402},
}

@article{gelman_bayesian_2020,
	title = {Bayesian Workflow},
	url = {http://arxiv.org/abs/2011.01808},
	abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
	urldate = {2020-11-04},
	journal = {arXiv:2011.01808 [stat]},
	author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.01808},
	keywords = {Statistics - Methodology},
}

@inproceedings{kay_when_2016,
	address = {San Jose California USA},
	title = {When (ish) is My Bus?: User-centered Visualizations of Uncertainty in Everyday, Mobile Predictive Systems},
	isbn = {978-1-4503-3362-7},
	shorttitle = {When (ish) is {My} {Bus}?},
	url = {https://dl.acm.org/doi/10.1145/2858036.2858558},
	doi = {10.1145/2858036.2858558},
	abstract = {Users often rely on realtime predictions in everyday contexts like riding the bus, but may not grasp that such predictions are subject to uncertainty. Existing uncertainty visualizations may not align with user needs or how they naturally reason about probability. We present a novel mobile interface design and visualization of uncertainty for transit predictions on mobile phones based on discrete outcomes. To develop it, we identified domain specific design requirements for visualizing uncertainty in transit prediction through: 1) a literature review, 2) a large survey of users of a popular realtime transit application, and 3) an iterative design process. We present several candidate visualizations of uncertainty for realtime transit predictions in a mobile context, and we propose a novel discrete representation of continuous outcomes designed for small screens, quantile dotplots. In a controlled experiment we find that quantile dotplots reduce the variance of probabilistic estimates by {\textasciitilde}1.15 times compared to density plots and facilitate more confident estimation by end-users in the context of realtime transit prediction scenarios.},
	language = {en},
	urldate = {2022-09-13},
	booktitle = {Proceedings of the 2016 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Kay, Matthew and Kola, Tara and Hullman, Jessica R. and Munson, Sean A.},
	month = may,
	year = {2016},
	pages = {5092--5103},
}

@article{wilkinson_dot_1999,
	title = {Dot plots},
	volume = {53},
	issn = {00031305},
	url = {https://www.jstor.org/stable/2686111?origin=crossref},
	doi = {10.2307/2686111},
	number = {3},
	urldate = {2023-04-12},
	journal = {The American Statistician},
	author = {Wilkinson, Leland},
	month = aug,
	year = {1999},
	pages = {276},
}

@article{gelman_posterior_1996,
	title = {Posterior predictive assessment of model fitness via realized discrepancies},
	volume = {6},
	url = {https://www.jstor.org/stable/24306036},
	number = {4},
	journal = {Statistica Sinica},
	author = {Gelman, Andrew and Xiao-Li, Meng and Stern, Hal S.},
	month = oct,
	year = {1996},
	pages = {733--760},
}

@Manual{package_cubature,
    title = {{cubature}: Adaptive Multivariate Integration over Hypercubes},
    author = {Balasubramanian Narasimhan and Steven G. Johnson and Thomas Hahn and Annie Bouvier and Kiên Kiêu},
    year = {2024},
    note = {R package version 2.1.1},
		doi = {10.32614/CRAN.package.cubature},
  }

@Article{package_caret,
    title = {Building Predictive Models in {R} Using the {caret} Package},
    volume = {28},
    url = {https://www.jstatsoft.org/index.php/jss/article/view/v028i05},
    doi = {10.18637/jss.v028.i05},
    number = {5},
    journal = {Journal of Statistical Software},
    author = {{Kuhn} and {Max}},
    year = {2008},
    pages = {1–26},
  }

  @Manual{package_iso,
    title = {{Iso}: Functions to Perform Isotonic Regression},
    author = {Rolf Turner},
    year = {2023},
    note = {R package version 0.0-21},
		doi={10.32614/CRAN.package.Iso},
  }
  @Misc{package_rstanarm,
    title = {{rstanarm}: {Bayesian} applied regression modeling via {Stan}.},
    author = {Ben Goodrich and Jonah Gabry and Imad Ali and Sam Brilleman},
    note = {R package version 2.32.1},
    year = {2024},
    url = {https://mc-stan.org/rstanarm/},
		doi={10.32614/CRAN.package.rstanarm},
  }
  @Article{package_brms1,
    title = {{brms}: An {R} Package for {Bayesian} Multilevel Models Using {Stan}},
    author = {Paul-Christian Bürkner},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {80},
    number = {1},
    pages = {1--28},
    doi = {10.18637/jss.v080.i01},
    encoding = {UTF-8},
  }
  @Article{package_brms2,
    title = {Advanced {Bayesian} Multilevel Modeling with the {R} Package {brms}},
    author = {Paul-Christian Bürkner},
    journal = {The R Journal},
    year = {2018},
    volume = {10},
    number = {1},
    pages = {395--411},
    doi = {10.32614/RJ-2018-017},
    encoding = {UTF-8},
  }
  @Article{package_brms3,
    title = {Bayesian Item Response Modeling in {R} with {brms} and {Stan}},
    author = {Paul-Christian Bürkner},
    journal = {Journal of Statistical Software},
    year = {2021},
    volume = {100},
    number = {5},
    pages = {1--54},
    doi = {10.18637/jss.v100.i05},
    encoding = {UTF-8},
  }


@Manual{package_cmdstanr,
  title = {{cmdstanr}: {R} Interface to {'CmdStan'}},
  author = {Jonah Gabry and Rok Češnovar and Andrew Johnson and Steve Bronder},
  year = {2024},
  note = {R package version 0.8.1, https://discourse.mc-stan.org},
  url = {https://mc-stan.org/cmdstanr/},
}

@Manual{package_r,
     title = {R: A Language and Environment for Statistical Computing},
     author = {{R Core Team}},
     organization = {R Foundation for Statistical Computing},
     address = {Vienna, Austria},
     year = {2021},
     url = {https://www.R-project.org/},
   }

@misc{standev2018stancore,
	title = {The {Stan} Core Library},
	author = {{Stan} Development Team},
	note = {Version 2.18.0},
	year = {2018},
	url = {http://mc-stan.org/},
}

@misc{gabry_plotting_2022,
  title = {{bayesplot}: Plotting for {Bayesian} Models},
  author = {Jonah Gabry and Tristan Mahr},
  year = {2024},
  note = {R package version 1.11.0},
  url = {https://mc-stan.org/bayesplot/},
	doi= {10.32614/CRAN.package.bayesplot},
}